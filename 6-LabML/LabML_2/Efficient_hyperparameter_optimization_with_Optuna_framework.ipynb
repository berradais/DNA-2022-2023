{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJ3R84qVJUyz"
      },
      "source": [
        "# Efficient hyperparameter optimization with Optuna framework: practical example\n",
        "\n",
        "---\n",
        "\n",
        "Hyperparameters optimization is an important part of working on data science projects. But the more parameters we have to optimize, the more difficult it is to do it manually. To speed up project development, we may want to automate this work. In order to do this, we will use **Optuna** framework.\n",
        "\n",
        "In this practical example of hyperparameters optimization, we will address a binary classification problem.\n",
        "\n",
        "*(based on http://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JQROnOoLHTz"
      },
      "source": [
        "## Let's get into practice!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsQiXjZ0LMC-"
      },
      "source": [
        "Install and import **Optuna**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zVRyGSGK430"
      },
      "source": [
        "!pip install optuna\n",
        "import optuna"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4euIO0OOJV_c"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import copy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOAP_slNLaOr"
      },
      "source": [
        "We will use the Ants vs Bees dataset, which is part of the ImageNet dataset. You will need to download it from here: [Ants vs Bees](https://download.pytorch.org/tutorial/hymenoptera_data.zip). It contains 400 pictures, ~250 training, and ~150 validation (test)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBaDcUK-GFVo"
      },
      "source": [
        "!wget https://download.pytorch.org/tutorial/hymenoptera_data.zip\n",
        "!unzip hymenoptera_data.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOQZwO_tGK7z"
      },
      "source": [
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "data_dir = './hymenoptera_data'\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in ['train', 'val']}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
        "                                             shuffle=True, num_workers=4)\n",
        "              for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pXza9BIMP9H"
      },
      "source": [
        "In order to get a pretrained model by its name, we will add a function *get_model*:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oUfPvZ9HLEk"
      },
      "source": [
        "def get_model(model_name: str = \"resnet18\"):  \n",
        "    if model_name == \"resnet18\":\n",
        "        model = models.resnet18(pretrained=True)\n",
        "        in_features = model.fc.in_features\n",
        "        model.fc = nn.Linear(in_features, 2)\n",
        "    elif model_name == \"alexnet\":\n",
        "        model = models.alexnet(pretrained=True)\n",
        "        in_features = model.classifier[1].in_features\n",
        "        model.classifier = nn.Linear(in_features, 2)\n",
        "    elif model_name == \"vgg16\":\n",
        "        model = models.vgg16(pretrained=True)\n",
        "        in_features = model.classifier[0].in_features\n",
        "        model.classifier = nn.Linear(in_features, 2)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ub5tz6odL-ss"
      },
      "source": [
        "The following function will be used to train the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ay4J5UTIHpBs"
      },
      "source": [
        "def train_model(trial, model, criterion, optimizer, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train() \n",
        "            else:\n",
        "                model.eval()  \n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "        \n",
        "        trial.report(epoch_acc, epoch)\n",
        "        if trial.should_prune():\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, best_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVAypp49Mwgr"
      },
      "source": [
        "We need to create the **Objective Function**. It takes a configuration of hyperparameters and returns its evaluation score (Objective value). By maximizing or minimizing the **Objective Function**, Optuna solves the problem of hyperparameter optimization.\n",
        "\n",
        "Within **Objective Function**, we should define the hyperparameters we want to optimize. In our example, we will optimize 3 hyperparameters:\n",
        "\n",
        "1.   Pretrained network. Since Ants vs Bees is a small dataset, we will use transfer learning to achieve a good quality model. We choose one of the networks trained on ImageNet and replace the last fully connected layers responsible for classification.\n",
        "2.   Optimizer: SGD, Adam.\n",
        "3.   Learning Rate: from 1e-4 to 1e-2.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0JK8jxlHW3W"
      },
      "source": [
        "def objective(trial):\n",
        "    \n",
        "    # Hyperparameters we want optimize\n",
        "    params = {\n",
        "        \"model_name\": trial.suggest_categorical('model_name',[\"resnet18\", \"alexnet\", \"vgg16\"]),\n",
        "        \"lr\": trial.suggest_loguniform('lr', 1e-4, 1e-2),\n",
        "        \"optimizer_name\": trial.suggest_categorical('optimizer_name',[\"SGD\", \"Adam\"])\n",
        "    }\n",
        "    \n",
        "    # Get pretrained model\n",
        "    model = get_model(params[\"model_name\"])\n",
        "    model = model.to(device)\n",
        "    \n",
        "    # Define criterion\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # Configure optimizer\n",
        "    optimizer = getattr(\n",
        "        torch.optim, params[\"optimizer_name\"]\n",
        "    )(model.parameters(), lr=params[\"lr\"])\n",
        "    \n",
        "    # Train model\n",
        "    best_model, best_acc = train_model(trial, model, criterion, optimizer, num_epochs=20)\n",
        "    \n",
        "    # Save best model for each trial\n",
        "    # torch.save(best_model.state_dict(), f\"model_trial_{trial.number}.pth\")\n",
        "    \n",
        "    # Return accuracy (Objective Value) of the current trial\n",
        "    return best_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g1hzClHNcgf"
      },
      "source": [
        "To start optimizing our **Objective Function**, we create a new **study**:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qEl_XlRHaO3"
      },
      "source": [
        "# sampler: We want to use a TPE sampler\n",
        "# pruner: We use a MedianPruner in order to interrupt unpromising trials\n",
        "# direction: The direction of study is “maximize” because we want to maximize the accuracy\n",
        "# n_trials: Number of trials\n",
        "\n",
        "sampler = optuna.samplers.TPESampler()    \n",
        "study = optuna.create_study(\n",
        "    sampler=sampler,\n",
        "    pruner=optuna.pruners.MedianPruner(\n",
        "        n_startup_trials=3, n_warmup_steps=5, interval_steps=3\n",
        "    ),\n",
        "    direction='maximize')\n",
        "study.optimize(func=objective, n_trials=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyrLHug3He93"
      },
      "source": [
        "print(\"Best trial: \")\n",
        "print(study.best_trial)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P29-NofZPFrG"
      },
      "source": [
        "Optuna helps to visually assess the impact of hyperparameters on the accuracy of the predictions. Let’s visualize the dependence between the learning rate, optimizer_name, model_name and accuracy (Objective Value):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPXvWiLtImVN"
      },
      "source": [
        "optuna.visualization.plot_parallel_coordinate(study)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KU5242KzIqHU"
      },
      "source": [
        "optuna.visualization.plot_contour(study, params=['optimizer_name','model_name'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6WQd4QCPKPB"
      },
      "source": [
        "Slice plots for each of the hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwvEQVf3IzOy"
      },
      "source": [
        "optuna.visualization.plot_slice(study)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fr2NyF8uPNWh"
      },
      "source": [
        "Hyperparameter importances:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJB8pjVoPP0P"
      },
      "source": [
        "optuna.visualization.plot_param_importances(study)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPoUWHEZPUgH"
      },
      "source": [
        "Plot the optimization history of all trials in a study:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAKdBLolIzkl"
      },
      "source": [
        "optuna.visualization.plot_optimization_history(study)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjMZqJMVPZyZ"
      },
      "source": [
        " Learning curves of the trials:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdXxZNlyI4NH"
      },
      "source": [
        "optuna.visualization.plot_intermediate_values(study)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}